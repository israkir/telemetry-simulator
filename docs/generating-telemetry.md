# Generating Telemetry Data

This guide describes how to generate realistic OpenTelemetry data with the telemetry simulator. Use this for pipeline testing, dashboard validation, and load testing.

## Overview

The simulator produces OTEL-compliant **traces**, **metrics**, and **logs** with configurable semantic conventions (attribute prefix set via `VENDOR`). It combines:

- **Semantic conventions**: Attribute definitions—names, types, and **allowed values**—come from the semantic conventions YAML (`SEMCONV` / `--semconv`). Emitted values for enum-like attributes (e.g. `error.type`, `step.outcome`) are always chosen from these allowed values so that both happy-path and error-path data are valid and queryable.
- **Realism**: Scenario-driven content (conversation samples, failure modes such as 4xx or wrong division, partial workflows) is configured in `config/config.yaml` and per-scenario YAML. Realistic modifiers inject the right attributes while staying within SemConv allowed values.
- **Randomness**: Which scenario runs, which conversation sample is used, latency, and optional probabilistic spans are varied using distributions and random choice over allowed values.

See [Realism, randomness, and semantic conventions](#realism-randomness-and-semantic-conventions) for how these three work together in detail.

- **Traces**: A tree of spans using the configured vendor prefix (e.g. `{prefix}.a2a.orchestrate` → `{prefix}.planner`, `{prefix}.mcp.tool.execute`, `{prefix}.llm.call`)
- **Metrics**: Correlated with spans (e.g. turn duration, tool call counts)
- **Logs**: Emitted for span events where configured

Tenant IDs come from `config/config.yaml` (`tenants`; scenarios set `context.tenant` by key, e.g. `toro`).

### Trace, session, and conversation IDs

| Identifier | Attribute / field | Semantics | How the simulator sets it |
|------------|-------------------|-----------|----------------------------|
| **Trace ID** | `trace_id` (span context) | One user turn with child spans. | Generated by the OTEL SDK per trace (32 hex chars). Each `generate_trace` call produces one trace and one trace_id. |
| **Session** | `{prefix}.session.id` | First-class; one logical interaction that may encompass **one or more traces**. All spans for that interaction carry the same value; in production propagated via Baggage. | For multi-turn, one session_id per logical session; the **same** value is set on **every span in every trace** that belongs to that session (so all turns are correlated). |
| **Conversation** (OTEL GenAI) | `gen_ai.conversation.id` | OTEL GenAI conversation identifier; **SHOULD equal** `{prefix}.session.id` for the same conversation. | Set to the same value as `{prefix}.session.id` on every span (so they match across all traces in the session). |

So: one trace = one trace_id (SDK); one logical session can have multiple traces (one per turn), all sharing the same session_id and gen_ai.conversation.id.

**How session and correlation IDs are generated:**

- **Config**: `config/config.yaml` defines **id_formats** for all correlation IDs: `session_id`, `conversation_id`, `request_id`, `mcp_tool_call_id`, `enduser_pseudo_id`. Placeholders: `{hex:N}`, `{uuid}`, `{tenant_id}`. Example: `session_id: "sess_toro_{hex:12}"`. **All IDs are generated from these templates only; there are no fallbacks.**
- **With ScenarioIdGenerator**: When a scenario has an id_generator (from the loader), the runner uses `id_gen.session_id()`, `id_gen.request_id()`, `id_gen.enduser_pseudo_id()` (and `id_gen.mcp_tool_call_id()` in the hierarchy) to produce values from config.
- **Without ScenarioIdGenerator**: The runner and `GenerationContext.create()` use standalone helpers (`generate_session_id()`, `generate_request_id()`, `generate_enduser_pseudo_id()`, etc.) that read the same **id_formats** from config. Same format, same single source of truth.
- **Where**: The runner calls `_context_kwargs_for_scenario(scenario, tenant_id)`, which sets `session_id`, `request_id`, and `user_id` (from id_gen or from the standalone generators) and passes them into `GenerationContext.create(**ctx_kwargs)`. That context is used for all spans in that logical request.
- **One session per iteration**: For `run_scenario`, each of the `repeat_count` iterations gets a **new** session_id. For `run_mixed_workload`, each step gets a **new** session_id.
- **Multi-turn**: When a scenario has `conversation.turns`, all turns in **one** iteration share the **same** session_id; `request_id` is refreshed per turn.

**Correlating multi-turn traces:** When a scenario has multiple conversation turns, each turn is emitted as a **separate trace** (different `trace_id`). To correlate them into one conversation:

- **Filter or group by `{prefix}.session.id`** (e.g. `vendor.session.id = 'sess_toro_abc123'`) — all spans from all traces in that session share this value.
- **Or use `gen_ai.conversation.id`** — same value on every span; equals session.id for the same conversation.

In Jaeger, Tempo, or your backend: query spans where `session.id` (or `gen_ai.conversation.id`) equals the session value to get the full multi-turn conversation across trace boundaries.

---

## Configuration reference (what exists)

The simulator is driven by **`src/simulator/scenarios/config/config.yaml`**. What exists today:

| Section | Purpose |
|--------|---------|
| **id_formats** | Templates for all correlation IDs: `session_id`, `conversation_id`, `request_id`, `mcp_tool_call_id`, `enduser_pseudo_id` (placeholders: `{hex:N}`, `{uuid}`, `{tenant_id}`). Used by `ScenarioIdGenerator` and by standalone generators when no id_generator; no fallbacks. |
| **happy_path_latency** | Mean and variance per span for realistic fluctuation: `default_variance` and `spans` (e.g. a2a_orchestrate, planner, tools_recommend, mcp_tool_execute, response_compose) with `mean_ms` and `variance`. Trace generator uses mean × (1 + gauss(0, variance)). When the hierarchy is built from context (e.g. `data_plane.workflow`), these values are used for each span type. |
| **tools_recommend** | Tool recommendation (tools.recommend span) attributes: `selection_strategy`, `selection_constraints`, `tools_selected_count`, `selection_fallback_used`. Together with MCP server tools list (mcp.tools.available.count) and happy_path_latency (mcp.selection.latency.ms), all tools.recommend span attributes come from config. |
| **mcp_retry** | `retry_policy` (e.g. none, exponential) and `latency_by_error_type` (timeout, unavailable, tool_error, etc.) for realistic latency on failed MCP attempts. Used when building MCP hierarchies from templates. |
| **mcp_retry_templates** | Named templates for MCP tool retry behavior (replaces random error_rate). Each template has `attempts`: list of `{ outcome: success \| failure, optional error_type, optional latency_mean_ms }`. Scenario `data_plane.mcp_retry` can reference a template by name or define `attempts` inline. When set, MCP spans get one child per attempt with deterministic outcomes. |
| **resource** | Resource attributes (service.name, service.version, deployment.environment.name, module, component, otel.source) and schema_url. |
| **tenants** | Key → id (e.g. `toro` → tenant UUID). Scenarios reference `context.tenant` by key. |
| **agents** | List of agents by `id` (e.g. `toro-customer-assistant-001`). Scenarios reference `context.agent` by id. No channel or division on agent; scenario sets `context.mcp_server`. |
| **mcp_servers** | Key → mcp_server_uuid + tools (name, tool_uuid). Scenarios reference `context.mcp_server` by key. |
| **realistic_scenarios** | `divisions` (division name → mcp_servers key), `error_templates` (simulation_goal → error_type, http_status_codes), `workflow_templates` (workflow name → list of steps). |
| *(no data_plane_templates)* | Data-plane is defined in each scenario YAML: `data_plane.workflow` (key into `workflow_templates` for step list), optional `data_plane.simulation_goal`, optional `data_plane.control_plane_template`, optional `data_plane.mcp_retry` (template name or inline `attempts` for MCP retry logic). Config supplies only `workflow_templates` (step names) and key → UUID. |
| **control_plane** | `trace_flow`, `request_validation_templates` (and optional `_defaults` for shared error_rate etc.), `response_validation_templates`, `latencies_ms`, and optional **request_scenarios** (name → template + description). Scenarios use `control_plane.template` or `request_outcome` + `block_reason`. When using the built-in definitions dir, any entry in `request_scenarios` is exposed as a scenario even without a YAML file; a YAML in `definitions/` overrides the registry. |
| **conversation_samples** | Per-workflow samples (user_input, llm_response) for single-turn content when scenario has no `conversation.turns`. |

Scenario YAML can also set **tags** (e.g. `control-plane`, `data-plane`, `happy-path`, `multi-turn`). Use `otelsim run --vendor=your_vendor --tags=...` to run only scenarios that have at least one of the given tags. Use `--each-once` to run each (tagged) scenario exactly once instead of `--count` random picks (e.g. `otelsim run --vendor=your_vendor --each-once` or `otelsim run --vendor=your_vendor --tags=control-plane --each-once`).

---

## Quick Start

### Prerequisites

- Python 3.11+
- **Schema path**: Set `SEMCONV` or pass `--semconv` (before or after the subcommand)
- OTLP endpoint (e.g. data-plane collector on port 4318)
- Tenant ID is read from `config/config.yaml` (`tenants`; scenario sets `context.tenant` by key); no env required.

### Run a Scenario

By default the simulator uses **sample scenario definitions** bundled in `src/simulator/scenarios/definitions/`. You can also pass a custom folder with `--scenarios-dir`.

```bash
# From the otelsim project directory
make venv && make install

# Run sample scenarios (built-in definitions; tenant from config/config.yaml)
otelsim scenario --vendor=your_vendor --name new_claim_phone
otelsim scenario --vendor=your_vendor --name request_blocked_by_policy

# Use your own scenario folder
otelsim scenario --vendor=your_vendor --name my_scenario --scenarios-dir /path/to/my/definitions

# Show full span output
otelsim scenario --vendor=your_vendor --name new_claim_phone --count 10 --show-full-spans
```

### Mixed Workload

Run all scenarios in random mix:

```bash
otelsim run --vendor=your_vendor --count 500 --interval 200
```

By default, mixed workloads sample uniformly across all loaded scenarios. You can bias the
selection using a **per-scenario workload weight**:

- **YAML**: add `workload_weight` at the top level of a scenario:

  ```yaml
  name: new_claim_phone
  tags: [data-plane, happy-path]
  workload_weight: 8.0   # 8x more likely than weight 1.0 (happy path = most traffic)
  ```

- **Control-plane registry** (`config/config.yaml` → `control_plane.request_scenarios`):

  ```yaml
  control_plane:
    request_scenarios:
      request_allowed_audit_flagged:
        template: allowed_but_flagged
        description: "Request allowed but flagged for audit..."
        workload_weight: 3.0
  ```

Semantics:

- `workload_weight` is a **non-negative relative weight** used when `otelsim run`
  randomly picks a scenario (including when `--tags` is set).
- A scenario with `workload_weight: 3.0` is **about 3× as likely** to be chosen as one
  with `workload_weight: 1.0` (all else equal).
- `workload_weight` defaults to **1.0** when omitted; values ≤ 0 effectively remove a
  scenario from random selection (but you can still run it via `otelsim scenario --vendor=your_vendor --name ...`).
- `--each-once` ignores weights and runs each (filtered) scenario exactly once.

Bundled scenario definitions use **realistic relative weights**: data-plane happy path (e.g. 8.0),
multi-turn and allowed-but-flagged lower (2–3), control-plane blocks and errors much lower (0.15–0.5),
so `otelsim run --vendor=your_vendor --count 1000` produces a mix that resembles real-world traffic (mostly success, some retries, minority blocks/errors).

---

## Example Scenarios

### Happy Path

Models a successful agent turn: user asks something, agent plans, calls a tool, returns an answer.

| Step | Span type (emitted) | Role |
|------|---------------------|------|
| 1    | `{prefix}.a2a.orchestrate` | Root; one user turn |
| 2    | `{prefix}.planner` | Planning / tool selection |
| 3    | `{prefix}.llm.call` | LLM call for tool selection |
| 4    | `{prefix}.mcp.tool.execute` + `{prefix}.mcp.tool.execute.attempt` | MCP tool call |
| 5    | `{prefix}.llm.call` | LLM call for final answer |
| 6    | `{prefix}.response.compose` | Response composition (direct child of root) |

```bash
otelsim scenario --vendor=your_vendor --name new_claim_phone
```

### Tool Retry (MCP retry then success)

Template-driven MCP retry: first attempt fails (e.g. timeout with realistic latency from config), second succeeds. Uses `data_plane.mcp_retry` and emits parent `mcp.tool.execute` with multiple `mcp.tool.execute.attempt` children, `retry.count`, `retry.policy`, `retry.reason`, and per-attempt outcomes.

```bash
otelsim scenario --vendor=your_vendor --name new_claim_phone_mcp_tool_retry_then_success
```

For statistical retry (distribution-based) in a scenario with a retry block, use a scenario that defines `retry.enabled` and retry config in its YAML; the bundled sample uses the template-driven MCP retry scenario above.

---

## Scenario Configuration

Scenarios support realistic generation features:

| Feature | Description |
|---------|-------------|
| **Log-normal latencies** | Right-skewed distributions matching real-world behavior |
| **Probabilistic spans** | Not all traces have all span types |
| **Count distributions** | Variable number of tool calls (e.g. Poisson) |
| **Error propagation** | Correlated failures through hierarchies |
| **Retry sequences** | Multi-attempt operations with backoff |
| **Attribute distributions** | Sample values from distributions |

### Example: Latency Distribution

```yaml
latency:
  distribution: log_normal
  median_ms: 200
  sigma: 0.8    # 0.5=tight, 0.8=moderate, 1.2=heavy tail
```

### Example: Probabilistic Span

```yaml
children:
  - type: rag.retrieve
    probability: 0.3    # Only 30% of traces include RAG
```

### Example: Retry Behavior

```yaml
retry:
  enabled: true
  max_attempts: 3
  force_initial_failure: true
  success_rate_per_attempt: [0.0, 0.75, 0.92]
```

### Realistic scenario-driven telemetry

| Simulation goal | Description | Telemetry effect |
|-----------------|-------------|------------------|
| `happy_path` | No failure injection | Default; no modifier. |
| `4xx_invalid_arguments` | Bad/missing parameters (wrong format, missing params) | One MCP attempt gets `error.type=invalid_arguments` and optional `http.response.status_code` in [400, 404, 422]. |
| `wrong_division` | Agent calls wrong division (same tool name, wrong server) | One MCP span uses a different division's `gentoro.mcp.server.uuid` (and tool UUID); attempt marked `error.type=tool_error`. |
| `partial_workflow` | Missing steps or wrong order | Hierarchy built from `actual_steps` (or `correct_flow.steps` with `skip_steps`); `response_compose` can be marked fail. |
| `ungrounded_response` | Ungrounded answers / bad summarization | `response_compose` span set to fail with `error.type=unavailable` (SemConv-aligned). |

**Config** (`config/config.yaml` → `realistic_scenarios`):

- **divisions**: Map division names (e.g. `phone`, `home_electronics`, `home_appliances`) to `mcp_servers` keys for wrong-division resolution.
- **error_templates**: Map each goal to `error_type` and optional `http_status_codes` (SemConv-aligned).

**Scenario YAML** (per definition):

- **simulation_goal**: One of the goals above.
- **realistic_overrides** (optional): e.g. `step_index_for_4xx`, `wrong_division_target`, `actual_steps`, `skip_steps`.

Example scenarios in `definitions/`: `new_claim_phone` (data-plane happy path), `new_claim_phone_multi_turn`, `new_claim_phone_mcp_tool_retry_then_success` (MCP retry: timeout then success), `request_blocked_by_policy`, `request_blocked_invalid_payload`, `request_blocked_rate_limited`, `request_blocked_invalid_payload_multi`, `request_allowed_audit_flagged`, `request_error_policy_runtime`, `request_blocked_policy_fail_closed`, `request_blocked_invalid_context_augment_exception`, `request_error_policy_unavailable` (control-plane-only).

Control-plane examples:

- `request_allowed_audit_flagged`: request allowed but flagged for audit (`gentoro.request.audit.flag=true`, `gentoro.policy.decision=allow_with_audit`).
- `request_blocked_rate_limited`: early block before policy (`gentoro.block.reason=rate_limited`; only root + payload spans).
- `request_blocked_invalid_payload_multi`: payload validation blocked with multiple `gentoro.validation.error` events on the payload span.
- `request_blocked_policy_fail_closed`: policy engine throws, but system is fail-closed (`gentoro.block.reason=request_policy`, `gentoro.policy.fail_mode=closed`; policy span ERROR with exception event).
- `request_blocked_invalid_context_augment_exception`: augmentation fails with exception → request blocked (`gentoro.block.reason=invalid_context`; augmentation span ERROR with AugmentationBindError event).

---

## Realism, randomness, and semantic conventions

The simulator combines **realistic scenario content**, **controlled randomness**, and **strict semantic conventions** so that generated telemetry is both varied and valid for pipelines and dashboards.

### How the three work together

| Layer | Purpose | Source of truth |
|-------|--------|-----------------|
| **Semantic conventions** | Attribute names, types, and **allowed values** (e.g. `error.type`, `step.outcome`) | `src/simulator/scenarios/conventions/semconv.yaml` (and code constants aligned with it) |
| **Realism** | Scenario-specific content: which failure to inject, which conversation to use, which division/server | `config/config.yaml` (`realistic_scenarios`, `conversation_samples`) and scenario YAML (`simulation_goal`, `context.workflow`, `conversation.turns`) |
| **Randomness** | Latency, which trace gets which scenario, which sample to pick, optional probabilistic spans | Distributions in scenario YAML, `random.choice` over allowed values or config samples |

All emitted values for **enum-like attributes** (e.g. `error.type`, `step.outcome`, `response.format`) are constrained to the semantic-convention allowed values. Randomness and realism only choose *within* those sets (or from configured samples that themselves use valid values).

### Semantic conventions (SemConv)

- **Schema**: The semantic-conventions YAML (`--semconv`) defines, per span type, which attributes exist and their `allowed_values`, `examples`, and `type`. The attribute generator uses this to decide what to emit and which values are valid.
- **Canonical enums in code**: For attributes that must stay low-cardinality and queryable, the simulator uses constants aligned with the schema:
  - **`error.type`**: Only `timeout`, `unavailable`, `invalid_arguments`, `tool_error`, `protocol_error` (from `SEMCONV_ERROR_TYPE_VALUES`). Any override or generated value that is not in this set is normalized to one of these before being set on a span.
  - **`step.outcome`**: Only `success`, `fail`, `skipped` (from `SEMCONV_STEP_OUTCOME_VALUES`) where applicable.
  - **`response.format`**: e.g. `a2a_json`, `a2a_stream`.
- **Error paths**: When a span is in error (`status.code=ERROR`), `error.type` is always one of the five values above (from the span-type default or from the realistic-scenario template). Retry and error-propagation logic also sample only from these types.
- **Happy path**: Success outcomes and response formats use the same allowed values so that traces remain consistent with validation and analytics.

### Realism

- **Realistic scenario goals** (`simulation_goal`): Each scenario can target a specific outcome (e.g. `4xx_invalid_arguments`, `wrong_division`, `ungrounded_response`). The modifier then injects the right attributes (e.g. `error.type=invalid_arguments`, wrong `mcp.server.uuid`, or `step.outcome=fail`) using **only** SemConv-aligned values from `realistic_scenarios.error_templates` in `config/config.yaml`.
- **Conversation messages**: Per convention we emit **one span per interaction** (user input → LLM response). Each LLM span carries `gen_ai.input.messages` = the user message(s) for *this* call and `gen_ai.output.messages` = the model reply for *this* call (no full conversation history on a single span). Content comes from:
  - **Scenario `conversation.turns`** when defined: use format `user_input` / `llm_response` per turn, or alternating `role`/`text`. Optional per turn: `user_input_redacted`, `llm_response_redacted` to define the redacted text for **gen_ai.input.redacted** / **gen_ai.output.redacted** when `redaction_applied` is not `none`. The simulator emits **one trace per turn**, all with the **same session_id** for that logical session.
  - **Scenario `conversation.samples`** when defined: one random sample per iteration (single-turn). Each sample can optionally include `user_input_redacted` and `llm_response_redacted` for defined redacted content when redaction is enabled.
  - **`conversation_samples`** in `config/config.yaml` when the scenario has a workflow and no `conversation.turns` or `conversation.samples`: one single-turn sample per workflow is chosen at random. Samples can optionally include `user_input_redacted` and `llm_response_redacted`.
- **Redaction**: Set **`redaction_applied`** in scenario YAML to `basic` or `strict` to enable content redaction (emits **gen_ai.input.redacted** and **gen_ai.output.redacted**). Default is `none` (no redacted attributes). When redaction is enabled, you can **define the redacted conversation text** in the scenario or config by providing `user_input_redacted` and `llm_response_redacted` alongside `user_input` and `llm_response`; otherwise the simulator auto-redacts from the same messages using pattern-based placeholders.
- **Wrong division / partial workflow**: Wrong-division scenarios swap `mcp.server.uuid` (and tool UUID) to another division from config; partial-workflow scenarios build the trace from `actual_steps` or `skip_steps`. In all cases, `error.type` and `step.outcome` on affected spans remain SemConv-aligned.

### Randomness

- **Which scenario**: In mixed workload (`otelsim run --vendor=your_vendor`), each trace picks a scenario at random from the loaded definitions (e.g. happy path, 4xx, wrong division, ungrounded).
- **Which sample**: When using `conversation_samples`, one user/LLM pair is chosen at random from the workflow’s `samples` list; when using `realistic_overrides`, optional indices (e.g. which MCP step gets 4xx) can be fixed or left random.
- **Latency and counts**: Scenario YAML can define distributions (e.g. log-normal latency, Poisson count) so that span durations and child counts vary realistically.
- **Attribute values**: For attributes that have schema `allowed_values`, the generator picks at random from those values when no override is provided. For `error.type` and `step.outcome`, the generator uses the SemConv constants so that randomness never produces an invalid enum.

### Summary

- **Realism** provides scenario-specific content (conversations, failure modes, wrong division, partial flows) from config and scenario YAML.
- **Randomness** provides variation (which scenario, which sample, latency, counts, and choice among allowed attribute values).
- **Semantic conventions** ensure that every emitted value for `error.type`, `step.outcome`, and other enum-like attributes is one of the allowed values in `src/simulator/scenarios/conventions/semconv.yaml`, so that both happy-path and error-path data are valid and queryable.

---

## Scenario File Structure

Scenarios are YAML files. The simulator ships with **sample definitions** in `src/simulator/scenarios/definitions/` (e.g. `new_claim_phone.yaml`, `new_claim_phone_multi_turn.yaml`, `new_claim_phone_mcp_tool_retry_then_success.yaml`, `request_blocked_by_policy.yaml`, `request_error_policy_runtime.yaml`). A reference file `example_scenario.yaml` documents all configuration options; it is excluded from `list` and from mixed workload when using the built-in samples, but you can run it explicitly with `--name example_scenario`. You can add your own YAML in the sample folder or use a **custom folder** via `--scenarios-dir` (or `SCENARIOS_DIR` with make). Example:

```yaml
name: my_scenario
description: Description of what this scenario tests

tags:
  - baseline
  - success

repeat_count: 100      # Number of traces to generate
interval_ms: 500       # Delay between traces

emit_metrics: true
emit_logs: true

root:
  type: vendor.a2a.orchestrate
  latency:
    distribution: log_normal
    median_ms: 1500
    sigma: 0.4
  error:
    rate: 0.02
  attributes:
    vendor.turn.status.code: SUCCESS

  children:
    - type: vendor.planner
      # ... nested spans
```

### Supported Span Types

Use `type` in scenario YAML with the same prefix as `VENDOR` (default `vendor`):

| Span type | Description |
|-----------|-------------|
| `{prefix}.a2a.orchestrate` | Root span for user turn |
| `{prefix}.planner` | Planning / tool selection |
| `{prefix}.task.execute` | Per sub-task (with `{prefix}.task.id`, `{prefix}.task.type`) |
| `{prefix}.llm.call` | LLM inference call |
| `{prefix}.mcp.tool.execute` | MCP tool (parent; child `{prefix}.mcp.tool.execute.attempt` per attempt) |
| `{prefix}.response.compose` | Response composition (with `{prefix}.response.format`, `{prefix}.step.outcome`) |
| `rag.retrieve` | RAG retrieval |
| `a2a.call` | Agent-to-agent call |
| `cp.request` | Control plane request |

Sample scenario YAML uses the default `vendor.*` namespace; the loader normalizes these to the `VENDOR` prefix at runtime.

---

## Running Scenarios

### CLI Commands

Global options (`--semconv`, `--endpoint`) can appear before or after the subcommand.

```bash
# Run specific scenario
otelsim scenario --vendor=your_vendor --name new_claim_phone

# Show spans as they're generated
otelsim scenario --vendor=your_vendor --name new_claim_phone --count 10 --show-spans

# Show full span details (all attributes)
otelsim scenario --vendor=your_vendor --name new_claim_phone --count 5 --show-full-spans

# Run mixed workload (all scenarios from default or custom folder)
otelsim run --vendor=your_vendor --count 500 --interval 200
otelsim run --vendor=your_vendor --count 500 --scenarios-dir /path/to/definitions

# Run only scenarios that have a given tag
otelsim run --vendor=your_vendor --count 100 --tags=control-plane
otelsim run --vendor=your_vendor --count 100 --tags=data-plane,multi-turn

# Run each (tagged) scenario exactly once (no random picks)
otelsim run --vendor=your_vendor --each-once
otelsim run --vendor=your_vendor --tags=control-plane --each-once
```

### Container

```bash
# From repo root
./tools/dev/dev deps-up

# Run in container (tenant from config/config.yaml in image)
podman run --rm \
  -e OTLP_HTTP_ENDPOINT=http://data-plane:4318 \
  your-image otelsim scenario --vendor=your_vendor --name new_claim_phone --count 200
```

### Make Target

```bash
otelsim run --vendor=your_vendor --semconv /path/to/conventions/semconv.yaml   # Mixed workload
```

----

## Adding New Scenarios

1. Create a YAML file either in the **sample definitions** folder (`src/simulator/scenarios/definitions/`) or in your own folder (then use `--scenarios-dir`). Example: `with_rag.yaml`

2. Define the scenario structure:
   ```yaml
   name: with_rag
   description: Agent turn with RAG retrieval
   
   repeat_count: 100
   interval_ms: 500
   
   root:
     type: vendor.a2a.orchestrate
     # ... your span hierarchy
   ```

3. Run it (use `--scenarios-dir` if your file is not in the sample definitions folder):
   ```bash
   otelsim scenario --vendor=your_vendor --name with_rag
   otelsim scenario --vendor=your_vendor --name with_rag --scenarios-dir /path/to/definitions
   ```

4. It will automatically be included in mixed workloads when you use the same folder (default or `--scenarios-dir`).

---

## What You Get

- **Traces**: One trace per repeat, with the defined hierarchy. Each span has realistic timing, status, and attributes.
- **Metrics**: Turn duration, tool call counts, LLM token usage, etc.
- **Logs**: Log records tied to spans where logging is enabled.

Data is sent to the configured OTLP endpoint (e.g. data-plane collector), which can export to Kafka, ClickHouse, or other backends.

---

## Live trace visualization

To view traces in a browser, run Jaeger and point the simulator at it (no data-plane container needed):

```bash
make jaeger-up
otelsim run --vendor=your_vendor --semconv /path/to/conventions/semconv.yaml
# Open http://localhost:16686, select service "otelsim"
make jaeger-down   # when done
```

See [Live Trace Visualization](./live-trace-visualization.md) for the full flow and options.

---

## See Also

- [Live Trace Visualization](./live-trace-visualization.md) - View traces in Jaeger UI
- [README](../README.md) - Quick start and CLI reference
